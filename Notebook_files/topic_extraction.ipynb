{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "import re\n",
    "#import gzip\n",
    "import collections\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora,models\n",
    "import json\n",
    "from collections import defaultdict\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMAZON_DATA_FILE_PATH = \"/Users/siva/Documents/CS506/datasets/Books_5.json\"\n",
    "def parse(path):\n",
    "    g = open(path, 'r')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "texts = []\n",
    "count = 0\n",
    "for i,l in enumerate(parse(AMAZON_DATA_FILE_PATH)):\n",
    "    tokens = tokenizer.tokenize(l[\"reviewText\"].lower())\n",
    "    tokens = [e for e in tokens if e not in stop_words]\n",
    "    #stemmed_tokens = [stemmer.stem(s) for s in tokens]\n",
    "    if len(tokens)>200:\n",
    "        count += 1\n",
    "        texts.append(tokens)\n",
    "    if count == 1:\n",
    "        break\n",
    "print(len(texts))\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "print(len(corpus))\n",
    "ldamodel = models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)\n",
    "\n",
    "print(ldamodel.print_topics(num_topics=2, num_words=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2400000\n",
      "2500000\n",
      "2600000\n",
      "2700000\n",
      "2800000\n",
      "2900000\n",
      "3000000\n",
      "3100000\n",
      "3200000\n",
      "3300000\n",
      "3400000\n",
      "3500000\n",
      "3600000\n",
      "3700000\n",
      "3800000\n",
      "3900000\n",
      "4000000\n",
      "4100000\n",
      "4200000\n",
      "4300000\n",
      "4400000\n",
      "4500000\n",
      "4600000\n",
      "4700000\n",
      "4800000\n",
      "4900000\n",
      "5000000\n",
      "5100000\n",
      "5200000\n"
     ]
    }
   ],
   "source": [
    "YELP_REVIEWS_DATA = \"/Users/siva/Documents/CS506/datasets/yelp_dataset/review.json\"\n",
    "business_reviews = defaultdict(list)\n",
    "with open(YELP_REVIEWS_DATA,\"r\") as f_json:\n",
    "    for i,line in enumerate(f_json):\n",
    "        review = json.loads(line)\n",
    "        business_reviews[review[\"business_id\"]].append(review[\"text\"])\n",
    "        if i % 100000 == 0:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10274\n"
     ]
    }
   ],
   "source": [
    "print(sum([1 for each in business_reviews if len(business_reviews[each])>100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "restaurants_with_topics = {}\n",
    "NO_OF_WORDS_IN_EACH_REVIEW_THRESHOLD = 3\n",
    "NO_OF_SENTENCES_IN_EACH_REVIEW_THRESHOLD = 7\n",
    "NO_OF_TOPICS = 2\n",
    "\n",
    "for i,each_business in enumerate(business_reviews):\n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "    if len(business_reviews[each_business]) > 100:\n",
    "        reviews_with_topics = defaultdict(list)\n",
    "        for each_review in business_reviews[each_business]:\n",
    "            sentences_each_review = each_review.split(\".\")\n",
    "            texts = []\n",
    "            for each_sentence in sentences_each_review:\n",
    "                tokens = tokenizer.tokenize(each_sentence.replace(\"\\n\",\"\").lower())\n",
    "                tokens = [e for e in tokens if e not in stop_words]\n",
    "                if len(tokens)>NO_OF_WORDS_IN_EACH_REVIEW_THRESHOLD:\n",
    "                    texts.append(tokens)\n",
    "            if len(texts) > NO_OF_SENTENCES_IN_EACH_REVIEW_THRESHOLD:\n",
    "                dictionary = corpora.Dictionary(texts)\n",
    "                corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "                ldamodel = models.ldamodel.LdaModel(corpus, num_topics=NO_OF_TOPICS, id2word = dictionary, passes=20)\n",
    "                topic = ldamodel.show_topics(num_topics = 1,num_words = 1,formatted = False)[0][1][0][0]\n",
    "                reviews_with_topics[topic].append(each_review)\n",
    "        restaurants_with_topics[each_business] = reviews_with_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results.json\",\"w\") as write_json:\n",
    "    for each in restaurants_with_topics:\n",
    "        json.dump(restaurants_with_topics[each],write_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
